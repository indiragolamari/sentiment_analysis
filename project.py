# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10naF8G3WA2TjehQzEv1ih_s-7X3igynz
"""

import pandas as pd
import numpy as np
import spacy
nlp=spacy.load('en_core_web_sm')
from wordcloud import wordcloud,STOPWORDS
import string
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.tokenize import word_tokenize,sent_tokenize
from sklearn.model_selection import GridSearchCV,train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from deep_translator import GoogleTranslator

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('vader_lexicon')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english')) - {'not', 'no', 'nor', 'never'}

df = pd.read_csv("P543.csv",on_bad_lines='skip',encoding='latin-1')

df.head()

df.isnull().sum()

df.duplicated().any()

df.shape

df.describe()

df['len']=df['body'].apply(len)

df

df.groupby('rating').describe()

# Combine title and body into one text column
df['text'] = df['title'] + " " + df['body']

#intilaize tools
stop_words= set(stopwords.words('english'))
lemmatizer= WordNetLemmatizer()

#So it removes:
#Emojis (üòä, ü§î, etc.)
#Numbers (123)
#Special characters (!, @, #, etc.)
#Punctuation (., ?, etc.)
#Non-English characters
# Define a function to clean and preprocess text
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = text.lower()  # Lowercase
    tokens = word_tokenize(text)  # Properly tokenize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

df['cleaned_text'] = df['text'].apply(clean_text)

# Create an empty list to store sentences and their ratings
sentence_data = []

#Iterate through the original DataFrame
for index, row in df.iterrows():
  rating = row['rating']  # Get the rating for the current document
  text = row['cleaned_text'] # Get the cleaned text for the current document
  sentences = sent_tokenize(text) # Tokenize the text into sentences
  # For each sentence in the document, append it and its rating to the list
  for sent in sentences:
    sentence_data.append({'text': sent, 'rating': rating})

#Create the DataFrame from the list of dictionaries
sent_df = pd.DataFrame(sentence_data)

sent_df

sent_df[['text', 'cleaned_text']].head(10)

def label_sentiment(rating):
    if rating <= 2:
        return 'negative'
    elif rating == 3:
        return 'neutral'
    else:
        return 'positive'

sent_df['sentiment'] = sent_df['rating'].apply(label_sentiment)

sent_df

sns.countplot(data=sent_df, x='sentiment')
plt.title("Distribution of Sentiments")
plt.show()

df['review_length'] = df['text'].apply(lambda x: len(x.split()))
sns.histplot(df['review_length'], bins=30)
plt.title("Review Length Distribution")
plt.xlabel("Number of Words")
plt.show()

from wordcloud import WordCloud

for sentiment in sent_df['sentiment'].unique():
    text = " ".join(sent_df[sent_df['sentiment'] == sentiment]['text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'WordCloud for {sentiment} Reviews')
    plt.show()

most_common_rating = sent_df['rating'].mode()[0]
print(f"The most frequent rating given by customers is: {most_common_rating}")

# we can  can also see the frequency of each rating
rating_counts = sent_df['rating'].value_counts()
print("\nFrequency of each rating:")
rating_counts

# finding the mean values of ratings
if 'rating' in df.columns:
    mean_of_rating = df['rating'].mean()
    print(f"The mean rating given by customers is: {mean_of_rating}")
else:
    print("The DataFrame does not contain a 'rating' column. Please check the column names.")
print("\033[94m*****This ratings tells us that The eproduct is above Average*****\033[0m")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

x_train, x_test, y_train, y_test = train_test_split(sent_df['text'], sent_df['sentiment'], test_size=0.1, random_state=42)

svm = Pipeline([
    ('tfidf_vectorizer', TfidfVectorizer(ngram_range=(1,2),stop_words=None,lowercase=True)),
    ('svm_classifier', SVC(probability=True))
])

svm.fit(x_train, y_train)

y_pred = svm.predict(x_test)
print(classification_report(y_test,y_pred))

accuracy_score(y_test,y_pred)

from sklearn.linear_model import LogisticRegression
lr = Pipeline([
    ('tfidf_vectorizer', TfidfVectorizer(ngram_range=(1,2),stop_words=None,lowercase=True)),
    ('linearregression', LogisticRegression())])

lr.fit(x_train, y_train)

y_pred = lr.predict(x_test)
print(classification_report(y_test,y_pred))

accuracy_score(y_test,y_pred)

import pandas as pd
import numpy as np
import joblib
import streamlit as st
import pickle
import warnings
warnings.filterwarnings('ignore')

import joblib
joblib.dump(svm, 'svm_pipeline.pkl')

import streamlit as st
import joblib

# Page Configuration
st.set_page_config(
    page_title="Sentiment Analyzer",
    page_icon="üìù",
    layout="centered"
)

# Custom CSS for styling
st.markdown("""
    <style>
        .main {
            background-color: #f9f9f9;
            padding: 2rem;
            border-radius: 10px;
        }
        h1 {
            color: #4a4a4a;
        }
        .stTextArea textarea {
            font-size: 16px;
        }
        .footer {
            text-align: center;
            color: gray;
            font-size: 13px;
            margin-top: 2rem;
        }
    </style>
""", unsafe_allow_html=True)

# Main Content
st.markdown('<div class="main">', unsafe_allow_html=True)

st.markdown("## üß† Review Sentiment Analyzer")
st.markdown("Enter your review below and let AI detect whether the sentiment is **Positive**, **Negative**, or **Neutral**!")

# Load the model
try:
    model = joblib.load("svm_pipeline.pkl")
    st.success("‚úÖ Model loaded successfully!")
except Exception as e:
    st.error(f"‚ùå Failed to load model: {e}")
    st.stop()

# User input
st.markdown("### ‚úçÔ∏è Your Review")
user_input = st.text_area("Type your review here:", height=150, placeholder="e.g., The product quality was amazing!")

# Prediction
if user_input.strip():
    prediction = model.predict([user_input])[0]

    st.markdown("### üîç Result")

    if prediction == 'positive':
        st.success("üòä Sentiment: **Positive**")
    elif prediction == 'negative':
        st.error("üôÅ Sentiment: **Negative**")
    else:
        st.warning("üòê Sentiment: **Neutral**")

    st.markdown("#### üóíÔ∏è Your Original Review")
    st.info(user_input)
else:
    st.info("üïµÔ∏è Please enter a review above to analyze sentiment.")

st.markdown("</div>", unsafe_allow_html=True)

# Footer
st.markdown('<div class="footer">Made with ‚ù§Ô∏è using Streamlit | SVM + TF-IDF Pipeline</div>', unsafe_allow_html=True)

